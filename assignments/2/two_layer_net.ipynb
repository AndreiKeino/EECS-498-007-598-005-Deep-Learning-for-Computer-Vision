{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "two_layer_net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zNmLmqrJAXXp"
      },
      "source": [
        "# EECS 498-007/598-005 Assignment 2-2: Two Layer Neural Network\n",
        "\n",
        "Before we start, please put your name and UMID in following format\n",
        "\n",
        ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tUGCJrp9Aegm"
      },
      "source": [
        "**Your Answer:**   \n",
        "Hello WORLD, #XXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hbe3wUpVAjma"
      },
      "source": [
        "## Implementing a Neural Network\n",
        "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. \n",
        "\n",
        "In other words, the network has the following architecture:\n",
        "\n",
        "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "Note: When you implment the regularization over W, please DO NOT multiply the regularization term by 1/2 (no coefficient). \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubB_0e-UAOVK"
      },
      "source": [
        "## Install starter code\n",
        "We will continue using the utility functions that we've used for Assignment 1: [`coutils` package](https://github.com/deepvision-class/starter-code). Run this cell to download and install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ASkY27ZtA7Is",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/deepvision-class/starter-code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z6WjZGY8A9CI"
      },
      "source": [
        "## Setup code\n",
        "Run some setup code for this notebook: Import some useful packages and increase the default figure size.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O3EvIZ0uAOVN",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import coutils\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# for plotting\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OvUDZWGU3VLV"
      },
      "source": [
        "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RrAX9FOLpr9k",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available:\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5T-4Phbd9GvI"
      },
      "source": [
        "The inputs to our network will be a batch of $N$ (`num_inputs`) $D$-dimensional vectors (`input_size`); the hidden layer will have $H$ hidden units (`hidden_size`), and we will predict classification scores for $C$ categories (`num_classes`). This means that the learnable weights and biases of the network will have the following shapes:\n",
        "\n",
        "*   W1: First layer weights; has shape (D, H)\n",
        "*   b1: First layer biases; has shape (H,)\n",
        "*   W2: Second layer weights; has shape (H, C)\n",
        "*   b2: Second layer biases; has shape (C,)\n",
        "\n",
        "We will use the following function to generate random weights for a small toy model while we implement the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NbG3r5CxGeEH",
        "colab": {}
      },
      "source": [
        "def get_toy_data(num_inputs=5, input_size=4, hidden_size=10, num_classes=3,\n",
        "                 dtype=torch.float32):\n",
        "  N = num_inputs\n",
        "  D = input_size\n",
        "  H = hidden_size\n",
        "  C = num_classes\n",
        " \n",
        "  # We set the random seed for repeatable experiments.\n",
        "  coutils.utils.fix_random_seed()\n",
        "  \n",
        "  # Generate some random parameters, storing them in a dict\n",
        "  params = {}\n",
        "  params['W1'] = 1e-4 * torch.randn(D, H, device='cuda').to(dtype)\n",
        "  params['b1'] = torch.zeros(H, device='cuda').to(dtype)\n",
        "  params['W2'] = 1e-4 * torch.randn(H, C, device='cuda').to(dtype)\n",
        "  params['b2'] = torch.zeros(C, device='cuda').to(dtype)\n",
        "\n",
        "  # Generate some random inputs and labels\n",
        "  toy_X = 10.0 * torch.randn(N, D, device='cuda').to(dtype)\n",
        "  toy_y = torch.tensor([0, 1, 2, 2, 1], dtype=torch.int64, device='cuda')\n",
        "  \n",
        "  return toy_X, toy_y, params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZLdCF3B-AOVT"
      },
      "source": [
        "## Forward pass: compute scores\n",
        "Like in the Linear Classifiers exercise, we want to write a function that takes as input the model weights and a batch of images and labels, and returns the loss and the gradient of the loss with respect to each model parameter.\n",
        "\n",
        "However rather than attempting to implement the entire function at once, we will take a staged approach and ask you to implement the full forward and backward pass one step at a time.\n",
        "\n",
        "First we will implement the forward pass of the network which uses the weights and biases to compute scores for all inputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oa5_On0t7Fgk",
        "colab": {}
      },
      "source": [
        "def nn_loss_part1(params, X, y=None, reg=0.0):\n",
        "    \"\"\"\n",
        "    The first stage of our neural network implementation: Run the forward pass\n",
        "    of the network to compute the hidden layer features and classification\n",
        "    scores. The network architecture should be:\n",
        "    \n",
        "    FC layer -> ReLU (hidden) -> FC layer (scores)\n",
        "\n",
        "    Inputs:\n",
        "    - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "      It should have following keys with shape\n",
        "          W1: First layer weights; has shape (D, H)\n",
        "          b1: First layer biases; has shape (H,)\n",
        "          W2: Second layer weights; has shape (H, C)\n",
        "          b2: Second layer biases; has shape (C,)\n",
        "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
        "      is not passed then we only return scores, and if it is passed then we\n",
        "      instead return the loss and gradients.\n",
        "    - reg: Regularization strength.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - scores: Tensor of shape (N, C) giving the classification scores for X\n",
        "    - hidden: Tensor of shape (N, H) giving the hidden layer representation\n",
        "      for each input value (after the ReLU).\n",
        "    \"\"\"\n",
        "    # Unpack variables from the params dictionary\n",
        "    W1, b1 = params['W1'], params['b1']\n",
        "    W2, b2 = params['W2'], params['b2']\n",
        "    N, D = X.shape\n",
        "\n",
        "    # Compute the forward pass\n",
        "    hidden = None\n",
        "    scores = None\n",
        "    #############################################################################\n",
        "    # TODO: Perform the forward pass, computing the class scores for the input. #\n",
        "    # Store the result in the scores variable, which should be an tensor of     #\n",
        "    # shape (N, C).                                                             #\n",
        "    #############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "    \n",
        "    return scores, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "inlH2l-XEtZQ"
      },
      "source": [
        "Compute the scores and compare with the answer. The distance gap should be smaller than 1e-10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZV9_3ZWAOVU",
        "colab": {}
      },
      "source": [
        "toy_X, toy_y, params = get_toy_data()\n",
        "\n",
        "scores, _ = nn_loss_part1(params, toy_X)\n",
        "print('Your scores:')\n",
        "print(scores)\n",
        "print()\n",
        "print('correct scores:')\n",
        "correct_scores = torch.tensor([\n",
        "        [-3.8160e-07,  1.9975e-07,  1.0911e-07],\n",
        "        [-5.0228e-08,  1.2784e-07, -5.2746e-08],\n",
        "        [-5.9560e-07,  9.1178e-07,  1.1879e-06],\n",
        "        [-3.2737e-08,  1.8820e-07, -2.8079e-07],\n",
        "        [-1.9523e-07,  2.0502e-07, -6.0692e-08]], dtype=torch.float32, device=scores.device)\n",
        "print(correct_scores)\n",
        "print()\n",
        "\n",
        "# The difference should be very small. We get < 1e-10\n",
        "scores_diff = (scores - correct_scores).abs().sum().item()\n",
        "print('Difference between your scores and correct scores: %.2e' % scores_diff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7XNJ3ydEAOVW"
      },
      "source": [
        "## Forward pass: compute loss\n",
        "Now, we implement the first part of `nn_loss_part2` that computes the data and regularization loss.\n",
        "\n",
        "For the data loss, we will use the softmax loss. For the regularization loss we will use L2 regularization on the weight matrices `W1` and `W2`; we will not apply regularization loss to the bias vectors `b1` and `b2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-jTtxJ1m8JIp",
        "colab": {}
      },
      "source": [
        "def nn_loss_part2(params, X, y=None, reg=0.0):\n",
        "    \"\"\"\n",
        "    Compute the loss and gradients for a two layer fully connected neural\n",
        "    network.\n",
        "\n",
        "    Inputs: Same as nn_loss_part1\n",
        "  \n",
        "    Returns:\n",
        "    If y is None, return a tensor scores of shape (N, C) where scores[i, c] is\n",
        "    the score for class c on input X[i].\n",
        "\n",
        "    If y is not None, instead return a tuple of:\n",
        "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "      samples.\n",
        "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "      with respect to the loss function; has the same keys as self.params.\n",
        "    \"\"\"\n",
        "    # Unpack variables from the params dictionary\n",
        "    W1, b1 = params['W1'], params['b1']\n",
        "    W2, b2 = params['W2'], params['b2']\n",
        "    N, D = X.shape\n",
        "\n",
        "    scores, h1 = nn_loss_part1(params, X, y, reg)\n",
        "    # If the targets are not given then jump out, we're done\n",
        "    if y is None:\n",
        "      return scores\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = None\n",
        "    #############################################################################\n",
        "    # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
        "    # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
        "    # in the variable loss, which should be a scalar. Use the Softmax           #\n",
        "    # classifier loss. When you implment the regularization over W, please DO   #\n",
        "    # NOT multiply the regularization term by 1/2 (no coefficient). If you are  #\n",
        "    # not careful here, it is easy to run into numeric instability (Check       #\n",
        "    # Numeric Stability in http://cs231n.github.io/linear-classify/).           #\n",
        "    #############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    grads = {}\n",
        "    #############################################################################\n",
        "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
        "    # and biases. Store the results in the grads dictionary. For example,       #\n",
        "    # grads['W1'] should store the gradient on W1, and be a tensor of same size #\n",
        "    #############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C734SdJGE6xh"
      },
      "source": [
        "First, implement the forward pass in the function `nn_loss_part2` above. Then run the following to check your implementation.\n",
        "\n",
        "We compute the loss for the toy data, and compare with the answer computed by our implementation. The difference between the correct and computed loss should be less than `1e-4`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wgG6w2uKAOVX",
        "colab": {}
      },
      "source": [
        "toy_X, toy_y, params = get_toy_data()\n",
        "\n",
        "loss, _ = nn_loss_part2(params, toy_X, toy_y, reg=0.05)\n",
        "print('Your loss: ', loss.item())\n",
        "correct_loss = 1.0986\n",
        "print('Correct loss: ', correct_loss)\n",
        "diff = (correct_loss - loss).item()\n",
        "\n",
        "# should be very small, we get < 1e-4\n",
        "print('Difference: %.4e' % diff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vExP-7n3AOVa"
      },
      "source": [
        "## Backward pass\n",
        "Now implement the backward pass for the entire network in `nn_loss_part2`.\n",
        "\n",
        "After doing so, we will use numeric gradient checking to see whether the analytic gradient computed by our backward pass mateches a numeric gradient.\n",
        "\n",
        "\n",
        "First we define a couple utility functions for our numeric gradient check:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CJitZg6cS8Sf",
        "colab": {}
      },
      "source": [
        "def compute_numeric_gradient(f, x, h=1e-7):\n",
        "  \"\"\" \n",
        "  Compute the numeric gradient of f at x using a finite differences\n",
        "  approximation. We use the centered difference:\n",
        "  \n",
        "  df/dx ~= (f(x + h) - f(x - h)) / (2 * h)\n",
        "  \n",
        "  Inputs:\n",
        "  - f: A function that inputs a torch tensor and returns a torch scalar\n",
        "  - x: A torch tensor giving the point at which to compute the gradient\n",
        "\n",
        "  Returns:\n",
        "  - grad: A tensor of the same shape as x giving the gradient of f at x\n",
        "  \"\"\" \n",
        "  fx = f(x) # evaluate function value at original point\n",
        "  flat_x = x.contiguous().view(-1)\n",
        "  grad = torch.zeros_like(x)\n",
        "  flat_grad = grad.view(-1)\n",
        "  # iterate over all indexes in x\n",
        "  for i in range(flat_x.shape[0]):\n",
        "    oldval = flat_x[i].item() # Store the original value\n",
        "    flat_x[i] = oldval + h    # Increment by h\n",
        "    fxph = f(x).item()        # Evaluate f(x + h)\n",
        "    flat_x[i] = oldval - h    # Decrement by h\n",
        "    fxmh = f(x).item()        # Evaluate f(x - h)\n",
        "    flat_x[i] = oldval        # Restore original value\n",
        "\n",
        "    # compute the partial derivative with centered formula\n",
        "    flat_grad[i] = (fxph - fxmh) / (2 * h)\n",
        "\n",
        "  return grad\n",
        "\n",
        "\n",
        "def rel_error(x, y, eps=1e-10):\n",
        "  \"\"\" returns relative error between x and y \"\"\"\n",
        "  top = (x - y).abs().max().item()\n",
        "  bot = (x.abs() + y.abs()).clamp(min=eps).max().item()\n",
        "  return top / bot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "93oOdibtW_Kl"
      },
      "source": [
        "Now we will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check.\n",
        "\n",
        "You should see relative errors less than `1e-4` for all parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCEkprvoAOVb",
        "colab": {}
      },
      "source": [
        "reg = 0.05\n",
        "toy_X, toy_y, params = get_toy_data(dtype=torch.float64)\n",
        "loss, grads = nn_loss_part2(params, toy_X, toy_y, reg=reg)\n",
        "\n",
        "for param_name, grad in grads.items():\n",
        "  param = params[param_name]\n",
        "  f = lambda w: nn_loss_part2(params, toy_X, toy_y, reg=reg)[0]\n",
        "  grad_numeric = compute_numeric_gradient(f, param)\n",
        "  error = rel_error(grad, grad_numeric)\n",
        "  print('%s max relative error: %e' % (param_name, error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LjAUalCBAOVd"
      },
      "source": [
        "## Train the network\n",
        "To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. Look at the function `nn_train` and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FOifd6qEFW_D",
        "colab": {}
      },
      "source": [
        "def nn_train(params, loss_func, pred_func, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=5e-6, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "  \"\"\"\n",
        "  Train this neural network using stochastic gradient descent.\n",
        "\n",
        "  Inputs:\n",
        "  - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "    It should have following keys with shape\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "  - loss_func: a loss function that computes the loss and the gradients.\n",
        "    It takes as input:\n",
        "    - params: Same as input to nn_train\n",
        "    - X_batch: A minibatch of inputs of shape (B, D)\n",
        "    - y_batch: Ground-truth labels for X_batch\n",
        "    - reg: Same as input to nn_train\n",
        "    And it returns a tuple of:\n",
        "      - loss: Scalar giving the loss on the minibatch\n",
        "      - grads: Dictionary mapping parameter names to gradients of the loss with\n",
        "        respect to the corresponding parameter.\n",
        "  - pred_func: prediction function that im\n",
        "  - X: A PyTorch tensor of shape (N, D) giving training data.\n",
        "  - y: A PyTorch tensor f shape (N,) giving training labels; y[i] = c means that\n",
        "    X[i] has label c, where 0 <= c < C.\n",
        "  - X_val: A PyTorch tensor of shape (N_val, D) giving validation data.\n",
        "  - y_val: A PyTorch tensor of shape (N_val,) giving validation labels.\n",
        "  - learning_rate: Scalar giving learning rate for optimization.\n",
        "  - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
        "    after each epoch.\n",
        "  - reg: Scalar giving regularization strength.\n",
        "  - num_iters: Number of steps to take when optimizing.\n",
        "  - batch_size: Number of training examples to use per step.\n",
        "  - verbose: boolean; if true print progress during optimization.\n",
        "  \n",
        "  Returns: A dictionary giving statistics about the training process\n",
        "  \"\"\"\n",
        "  num_train = X.shape[0]\n",
        "  iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "\n",
        "  # Use SGD to optimize the parameters in self.model\n",
        "  loss_history = []\n",
        "  train_acc_history = []\n",
        "  val_acc_history = []\n",
        "\n",
        "  for it in range(num_iters):\n",
        "    X_batch = None\n",
        "    y_batch = None\n",
        "\n",
        "    #########################################################################\n",
        "    # TODO: Create a random minibatch of training data and labels, storing  #\n",
        "    # them in X_batch and y_batch respectively.                             #\n",
        "    # hint: torch.randint                                                   #\n",
        "    #########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    #########################################################################\n",
        "    #                             END OF YOUR CODE                          #\n",
        "    #########################################################################\n",
        "\n",
        "    # Compute loss and gradients using the current minibatch\n",
        "    loss, grads = loss_func(params, X_batch, y=y_batch, reg=reg)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    #########################################################################\n",
        "    # TODO: Use the gradients in the grads dictionary to update the         #\n",
        "    # parameters of the network (stored in the dictionary self.params)      #\n",
        "    # using stochastic gradient descent. You'll need to use the gradients   #\n",
        "    # stored in the grads dictionary defined above.                         #\n",
        "    #########################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    #########################################################################\n",
        "    #                             END OF YOUR CODE                          #\n",
        "    #########################################################################\n",
        "\n",
        "    if verbose and it % 100 == 0:\n",
        "      print('iteration %d / %d: loss %f' % (it, num_iters, loss.item()))\n",
        "\n",
        "    # Every epoch, check train and val accuracy and decay learning rate.\n",
        "    if it % iterations_per_epoch == 0:\n",
        "      # Check accuracy\n",
        "      y_train_pred = pred_func(params, loss_func, X_batch)\n",
        "      train_acc = (y_train_pred == y_batch).float().mean().item()\n",
        "      y_val_pred = pred_func(params, loss_func, X_val)\n",
        "      val_acc = (y_val_pred == y_val).float().mean().item()\n",
        "      train_acc_history.append(train_acc)\n",
        "      val_acc_history.append(val_acc)\n",
        "\n",
        "      # Decay learning rate\n",
        "      learning_rate *= learning_rate_decay\n",
        "\n",
        "  return {\n",
        "    'loss_history': loss_history,\n",
        "    'train_acc_history': train_acc_history,\n",
        "    'val_acc_history': val_acc_history,\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lmRaOIc3MBf0"
      },
      "source": [
        "You will also have to implement `nn_predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cztJWzg0GWog",
        "colab": {}
      },
      "source": [
        "def nn_predict(params, loss_func, X):\n",
        "  \"\"\"\n",
        "  Use the trained weights of this two-layer network to predict labels for\n",
        "  data points. For each data point we predict scores for each of the C\n",
        "  classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "  Inputs:\n",
        "  - params: a dictionary of PyTorch Tensor that store the weights of a model.\n",
        "    It should have following keys with shape\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "  - loss_func: a loss function that computes the loss and the gradients\n",
        "  - X: A PyTorch tensor of shape (N, D) giving N D-dimensional data points to\n",
        "    classify.\n",
        "\n",
        "  Returns:\n",
        "  - y_pred: A PyTorch tensor of shape (N,) giving predicted labels for each of\n",
        "    the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
        "    to have class c, where 0 <= c < C.\n",
        "  \"\"\"\n",
        "  y_pred = None\n",
        "\n",
        "  ###########################################################################\n",
        "  # TODO: Implement this function; it should be VERY simple!                #\n",
        "  ###########################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  pass\n",
        "  ###########################################################################\n",
        "  #                              END OF YOUR CODE                           #\n",
        "  ###########################################################################\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cO1IJsdTFphL"
      },
      "source": [
        "Once you have implemented the method, run the code below to train a two-layer network on toy data. Your final training loss should be less than 1.05."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wgw06cLXAOVd",
        "colab": {}
      },
      "source": [
        "toy_X, toy_y, params = get_toy_data()\n",
        "\n",
        "stats = nn_train(params, nn_loss_part2, nn_predict, toy_X, toy_y, toy_X, toy_y,\n",
        "                 learning_rate=1e-1, reg=1e-6,\n",
        "                 num_iters=200, verbose=False)\n",
        "\n",
        "print('Final training loss: ', stats['loss_history'][-1])\n",
        "\n",
        "# plot the loss history\n",
        "plt.plot(stats['loss_history'], 'o')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('training loss')\n",
        "plt.title('Training Loss history')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EUS4aDp_HzG1",
        "colab": {}
      },
      "source": [
        "# Plot the loss function and train / validation accuracies\n",
        "plt.plot(stats['train_acc_history'], 'o', label='train')\n",
        "plt.plot(stats['val_acc_history'], 'o', label='val')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Clasification accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cq-HkgRBAOVQ"
      },
      "source": [
        "## Wrap all function into a Class\n",
        "We will use the class `TwoLayerNet` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are PyTorch tensors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W5Y_VPjrBWYu",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(object):\n",
        "  def __init__(self, input_size, hidden_size, output_size, device='cuda',\n",
        "               std=1e-4):\n",
        "    \"\"\"\n",
        "    Initialize the model. Weights are initialized to small random values and\n",
        "    biases are initialized to zero. Weights and biases are stored in the\n",
        "    variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "    W1: First layer weights; has shape (D, H)\n",
        "    b1: First layer biases; has shape (H,)\n",
        "    W2: Second layer weights; has shape (H, C)\n",
        "    b2: Second layer biases; has shape (C,)\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: The dimension D of the input data.\n",
        "    - hidden_size: The number of neurons H in the hidden layer.\n",
        "    - output_size: The number of classes C.\n",
        "    \"\"\"\n",
        "    # fix random seed before we generate a set of parameters\n",
        "    coutils.utils.fix_random_seed()\n",
        "\n",
        "    self.params = {}\n",
        "    self.params['W1'] = std * torch.randn(input_size, hidden_size, device=device)\n",
        "    self.params['b1'] = torch.zeros(hidden_size, device=device)\n",
        "    self.params['W2'] = std * torch.randn(hidden_size, output_size, device=device)\n",
        "    self.params['b2'] = torch.zeros(output_size, device=device)\n",
        "\n",
        "  def _loss(self, params, X, y=None, reg=0.0):\n",
        "    return nn_loss_part2(params, X, y, reg)\n",
        "  \n",
        "  def loss(self, X, y=None, reg=0.0):\n",
        "    return self._loss(self.params, X, y, reg)\n",
        "\n",
        "\n",
        "  def _train(self, params, loss_func, pred_func, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=5e-6, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    return nn_train(params, loss_func, pred_func, X, y, X_val, y_val,\n",
        "            learning_rate, learning_rate_decay,\n",
        "            reg, num_iters, batch_size, verbose)\n",
        "    \n",
        "  def train(self, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=5e-6, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    return self._train(self.params, self._loss, self._predict, \n",
        "                       X, y, X_val, y_val,\n",
        "                       learning_rate, learning_rate_decay,\n",
        "                       reg, num_iters, batch_size, verbose)\n",
        "\n",
        "  def _predict(self, params, loss_func, X):\n",
        "    return nn_predict(params, loss_func, X)\n",
        "  \n",
        "  def predict(self, X):\n",
        "    return self._predict(self.params, self._loss, X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8cPIajWNAOVg"
      },
      "source": [
        "## Load CIFAR-10 data\n",
        "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lYo_XrU3AOVg",
        "colab": {}
      },
      "source": [
        "def get_CIFAR10_data(validation_ratio = 0.05):\n",
        "  \"\"\"\n",
        "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "  it for the linear classifier. These are the same steps as we used for the\n",
        "  SVM, but condensed to a single function.  \n",
        "  \"\"\"\n",
        "  X_train, y_train, X_test, y_test = coutils.data.cifar10()\n",
        "\n",
        "  # load every data on cuda\n",
        "  X_train = X_train.cuda()\n",
        "  y_train = y_train.cuda()\n",
        "  X_test = X_test.cuda()\n",
        "  y_test = y_test.cuda()\n",
        "\n",
        "  # 0. Visualize some examples from the dataset.\n",
        "  class_names = [\n",
        "      'plane', 'car', 'bird', 'cat', 'deer',\n",
        "      'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "  ]\n",
        "  img = coutils.utils.visualize_dataset(X_train, y_train, 12, class_names)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  # 1. Normalize the data: subtract the mean RGB (zero mean)\n",
        "  mean_image = X_train.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
        "  X_train -= mean_image\n",
        "  X_test -= mean_image\n",
        "\n",
        "  # 2. Reshape the image data into rows\n",
        "  X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "  X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "  # 3. take the validation set from the training set\n",
        "  # Note: It should not be taken from the test set\n",
        "  # For random permumation, you can use torch.randperm or torch.randint\n",
        "  # But, for this homework, we use slicing instead.\n",
        "  num_training = int( X_train.shape[0] * (1.0 - validation_ratio) )\n",
        "  num_validation = X_train.shape[0] - num_training\n",
        "\n",
        "  # return the dataset\n",
        "  data_dict = {}\n",
        "  data_dict['X_val'] = X_train[num_training:num_training + num_validation]\n",
        "  data_dict['y_val'] = y_train[num_training:num_training + num_validation]\n",
        "  data_dict['X_train'] = X_train[0:num_training]\n",
        "  data_dict['y_train'] = y_train[0:num_training]\n",
        "\n",
        "  data_dict['X_test'] = X_test\n",
        "  data_dict['y_test'] = y_test\n",
        "  return data_dict\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "data_dict = get_CIFAR10_data()\n",
        "print('Train data shape: ', data_dict['X_train'].shape)\n",
        "print('Train labels shape: ', data_dict['y_train'].shape)\n",
        "print('Validation data shape: ', data_dict['X_val'].shape)\n",
        "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
        "print('Test data shape: ', data_dict['X_test'].shape)\n",
        "print('Test labels shape: ', data_dict['y_test'].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_CsYAv3uAOVi"
      },
      "source": [
        "## Train a network\n",
        "To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hgg0QV9DAOVj",
        "colab": {}
      },
      "source": [
        "input_size = 3 * 32 * 32\n",
        "hidden_size = 36\n",
        "num_classes = 10\n",
        "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Train the network\n",
        "stats = net.train(data_dict['X_train'], data_dict['y_train'],\n",
        "                  data_dict['X_val'], data_dict['y_val'],\n",
        "                  num_iters=500, batch_size=1000,\n",
        "                  learning_rate=1e-2, learning_rate_decay=0.95,\n",
        "                  reg=0.25, verbose=True)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = net.predict(data_dict['X_val'])\n",
        "val_acc = 100.0 * (y_val_pred == data_dict['y_val']).float().mean().item()\n",
        "print('Validation accuracy: %.2f%%' % val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ixxgq5RKAOVl"
      },
      "source": [
        "## Debug the training\n",
        "With the default parameters we provided above, you should get a validation accuracy of about 8.76% on the validation set. This isn't very good.\n",
        "\n",
        "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
        "\n",
        "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6sYXImDTAOVm",
        "colab": {}
      },
      "source": [
        "# Plot the loss function and train / validation accuracies\n",
        "def plot_stats(stat_dict):\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(stat_dict['loss_history'], 'o')\n",
        "  plt.title('Loss history')\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Loss')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(stat_dict['train_acc_history'], 'o-', label='train')\n",
        "  plt.plot(stat_dict['val_acc_history'], 'o-', label='val')\n",
        "  plt.title('Classification accuracy history')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Clasification accuracy')\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.gcf().set_size_inches(14, 4)\n",
        "  plt.show()\n",
        "\n",
        "plot_stats(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "616EK5UoKgmE"
      },
      "source": [
        "Similar to SVM and Softmax classifier, let's visualize the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FnuRjtyKAOVo",
        "colab": {}
      },
      "source": [
        "def visualize_grid(Xs, ubound=255.0, padding=1):\n",
        "  \"\"\"\n",
        "  Reshape a 4D tensor of image data to a grid for easy visualization.\n",
        "\n",
        "  Inputs:\n",
        "  - Xs: Data of shape (N, H, W, C)\n",
        "  - ubound: Output grid will have values scaled to the range [0, ubound]\n",
        "  - padding: The number of blank pixels between elements of the grid\n",
        "  \"\"\"\n",
        "  (N, H, W, C) = Xs.shape\n",
        "  # print(Xs.shape)\n",
        "  grid_size = int(math.ceil(math.sqrt(N)))\n",
        "  grid_height = H * grid_size + padding * (grid_size - 1)\n",
        "  grid_width = W * grid_size + padding * (grid_size - 1)\n",
        "  grid = torch.zeros((grid_height, grid_width, C), device=Xs.device)\n",
        "  next_idx = 0\n",
        "  y0, y1 = 0, H\n",
        "  for y in range(grid_size):\n",
        "    x0, x1 = 0, W\n",
        "    for x in range(grid_size):\n",
        "      if next_idx < N:\n",
        "        img = Xs[next_idx]\n",
        "        low, high = torch.min(img), torch.max(img)\n",
        "        grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
        "        # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
        "        next_idx += 1\n",
        "      x0 += W + padding\n",
        "      x1 += W + padding\n",
        "    y0 += H + padding\n",
        "    y1 += H + padding\n",
        "  # print(grid.shape)\n",
        "  return grid\n",
        "\n",
        "\n",
        "# Visualize the weights of the network\n",
        "def show_net_weights(net):\n",
        "  W1 = net.params['W1']\n",
        "  W1 = W1.reshape(3, 32, 32, -1).transpose(0, 3)\n",
        "  plt.imshow(visualize_grid(W1, padding=3).type(torch.uint8).cpu())\n",
        "  plt.gca().axis('off')\n",
        "  plt.show()\n",
        "\n",
        "show_net_weights(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OlVbXxmPNzPY"
      },
      "source": [
        "## What's wrong?\n",
        "Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rDNZ8ZAnN7hj"
      },
      "source": [
        "### Capacity?\n",
        "Our initial model has very similar performance on the training and validation sets. This suggests that the model is underfitting, and that its performance might improve if we were to increase its capacity.\n",
        "\n",
        "One way we can increase the capacity of a neural network model is to increase the size of its hidden layer. Here we investigate the effect of increasing the size of the hidden layer. The performance (as measured by validation-set accuracy) should increase as the size of the hidden layer increases; however it may show diminishing returns for larger layer sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4TsWO7NqZPkR",
        "colab": {}
      },
      "source": [
        "def plot_acc_curves(stat_dict):\n",
        "  plt.subplot(1, 2, 1)\n",
        "  for key, single_stats in stat_dict.items():\n",
        "    plt.plot(single_stats['train_acc_history'], label=str(key))\n",
        "  plt.title('Train accuracy history')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Clasification accuracy')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  for key, single_stats in stat_dict.items():\n",
        "    plt.plot(single_stats['val_acc_history'], label=str(key))\n",
        "  plt.title('Validation accuracy history')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Clasification accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.gcf().set_size_inches(14, 5)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_C-ChHUlN68f",
        "colab": {}
      },
      "source": [
        "hidden_sizes = [2, 8, 32, 128, 512] \n",
        "lr = 0.1\n",
        "reg = 0.001\n",
        "\n",
        "stat_dict = {}\n",
        "for hs in hidden_sizes:\n",
        "  print('train with hidden size: {}'.format(hs))\n",
        "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device)\n",
        "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
        "            num_iters=3000, batch_size=1000,\n",
        "            learning_rate=lr, learning_rate_decay=0.95,\n",
        "            reg=reg, verbose=False)\n",
        "  stat_dict[hs] = stats\n",
        "\n",
        "plot_acc_curves(stat_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QpSrK3olUfOZ"
      },
      "source": [
        "### Regularization?\n",
        "Another possible explanation for the small gap we saw between the train and validation accuracies of our model is regularization. In particular, if the regularization coefficient were too high then the model may be unable to fit the training data.\n",
        "\n",
        "We can investigate the phenomenon empirically by training a set of models with varying regularization strengths while fixing other hyperparameters.\n",
        "\n",
        "You should see that setting the regularization strength too high will harm the validation-set performance of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DRPsxxFnU3Un",
        "colab": {}
      },
      "source": [
        "hs = 128\n",
        "lr = 1.0\n",
        "regs = [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "\n",
        "stat_dict = {}\n",
        "for reg in regs:\n",
        "  print('train with regularization: {}'.format(reg))\n",
        "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device)\n",
        "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
        "            num_iters=3000, batch_size=1000,\n",
        "            learning_rate=lr, learning_rate_decay=0.95,\n",
        "            reg=reg, verbose=False)\n",
        "  stat_dict[reg] = stats\n",
        "\n",
        "plot_acc_curves(stat_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3zFWkxebWXtu"
      },
      "source": [
        "### Learning Rate?\n",
        "Last but not least, we also want to see the effect of learning rate with respect to the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lc_YYCDmWld-",
        "colab": {}
      },
      "source": [
        "hs = 128\n",
        "lrs = [1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
        "reg = 1e-4\n",
        "\n",
        "stat_dict = {}\n",
        "for lr in lrs:\n",
        "  print('train with learning rate: {}'.format(lr))\n",
        "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device)\n",
        "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
        "            num_iters=3000, batch_size=1000,\n",
        "            learning_rate=lr, learning_rate_decay=0.95,\n",
        "            reg=reg, verbose=False)\n",
        "  stat_dict[lr] = stats\n",
        "\n",
        "plot_acc_curves(stat_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mVCEro4FAOVq"
      },
      "source": [
        "## Tune your hyperparameters\n",
        "\n",
        "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, number of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
        "\n",
        "**Plots**. To guide your hyperparameter search, you might consider making auxiliary plots of training and validation performance as above, or plotting the results arising from different hyperparameter combinations as we did in the Linear Classifier notebook. You should feel free to plot any auxiliary results you need in order to find a good network, but we don't require any particular plots from you.\n",
        "\n",
        "**Approximate results**. To get full credit for the assignment, you should achieve a classification accuracy above 50% on the validation set.\n",
        "\n",
        "(Our best model gets a validation-set accuracy above 58% -- did you beat us?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bG4DjBMIAOVq",
        "colab": {}
      },
      "source": [
        "best_net = None # store the best model into this \n",
        "\n",
        "#################################################################################\n",
        "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
        "# model in best_net.                                                            #\n",
        "#                                                                               #\n",
        "# To help debug your network, it may help to use visualizations similar to the  #\n",
        "# ones we used above; these visualizations will have significant qualitative    #\n",
        "# differences from the ones we saw above for the poorly tuned network.          #\n",
        "#                                                                               #\n",
        "# Tweaking hyperparameters by hand can be fun, but you might find it useful to  #\n",
        "# write code to sweep through possible combinations of hyperparameters          #\n",
        "# automatically like we did on the previous exercises.                          #\n",
        "#################################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "pass\n",
        "#################################################################################\n",
        "#                               END OF YOUR CODE                                #\n",
        "#################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NsYIu49plJ9r",
        "colab": {}
      },
      "source": [
        "# Check the validation-set accuracy of your best model\n",
        "y_val_preds = best_net.predict(data_dict['X_val'])\n",
        "val_acc = 100 * (y_val_preds == data_dict['y_val']).float().mean().item()\n",
        "print('Best val-set accuracy: %.2f%%' % val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hZgDq4zlAOVt",
        "colab": {}
      },
      "source": [
        "# visualize the weights of the best network\n",
        "show_net_weights(best_net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UG56gKWsAOVv"
      },
      "source": [
        "## Run on the test set\n",
        "When you are done experimenting, you should evaluate your final trained network on the test set. To get full credit for the assignment, you should achieve over 50% classification accuracy on the test set.\n",
        "\n",
        "(Our best model gets 54.1% test-set accuracy -- did you beat us?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2b3h8f8_AOVw",
        "colab": {}
      },
      "source": [
        "y_test_preds = best_net.predict(data_dict['X_test'])\n",
        "test_acc = 100 * (y_test_preds == data_dict['y_test']).float().mean().item()\n",
        "print('Test accuracy: %.2f%%' % test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}