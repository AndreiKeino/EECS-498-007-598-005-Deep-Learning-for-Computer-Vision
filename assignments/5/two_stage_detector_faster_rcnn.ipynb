{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "two_stage_detector_faster_rcnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DDJwQPZcupab"
      },
      "source": [
        "# EECS 498-007/598-005 Assignment 5-2: Two-Stage Object Detector - Faster R-CNN\n",
        "\n",
        "Before we start, please put your name and UMID in following format\n",
        "\n",
        ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2KMxqLt1h2kx"
      },
      "source": [
        "**Your Answer:**   \n",
        "Hello WORLD, #XXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PoRTyUc94S1a"
      },
      "source": [
        "# Two-Stage Object Detector\n",
        "In this exercise you will implement a **two-stage** object detector, based on [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), which consists of two modules, Region Proposal Networks (RPN) and Fast R-CNN. We will later use it to train a model that can detect objects on novel images and evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfBk3NtRgqaV"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubB_0e-UAOVK"
      },
      "source": [
        "## Install starter code\n",
        "We will continue using the utility functions that we've used for previous assignments: [`coutils` package](https://github.com/deepvision-class/starter-code). Run this cell to download and install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ASkY27ZtA7Is",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/deepvision-class/starter-code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MzqbYcKdz6ew"
      },
      "source": [
        "## Setup code\n",
        "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HzRdJ3uhe1CR",
        "tags": [
          "pdf-ignore"
        ],
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import coutils\n",
        "from coutils import extract_drive_file_id, register_colab_notebooks, \\\n",
        "                    fix_random_seed, rel_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# data type and device for torch.tensor\n",
        "to_float = {'dtype': torch.float, 'device': 'cpu'}\n",
        "to_float_cuda = {'dtype': torch.float, 'device': 'cuda'}\n",
        "to_double = {'dtype': torch.double, 'device': 'cpu'}\n",
        "to_double_cuda = {'dtype': torch.double, 'device': 'cuda'}\n",
        "to_long = {'dtype': torch.long, 'device': 'cpu'}\n",
        "to_long_cuda = {'dtype': torch.long, 'device': 'cuda'}\n",
        "\n",
        "# for mAP evaluation\n",
        "!rm -rf mAP\n",
        "!git clone https://github.com/Cartucho/mAP.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OvUDZWGU3VLV"
      },
      "source": [
        "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RrAX9FOLpr9k",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available:\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Saw9jGNm-9-"
      },
      "source": [
        "## Import functions from previous notebook\n",
        "Like what you have seen in the previous assignment, this notebook will re-use some pieces of code that you implemented in the previous notebook.\n",
        "\n",
        "In order to do this, you will need the **Google Drive file ID** of your completed notebook `single_stage_detector_yolo.ipynb`. You can find the this file ID by doing the following:\n",
        "1. Make sure you have saved your completed `single_stage_detector_yolo.ipynb` notebook to your own Google Drive\n",
        "2. Open you finished `single_stage_detector_yolo.ipynb` notebook in Colab.\n",
        "3. Click the \"Share\" button at the top of the screen\n",
        "4. Copy the \"Notebook link\" and paste it in the following cell, assigning it to the `YOLO_NOTEBOOK_LINK` variable\n",
        "\n",
        "**Important:** If you modify the implementations of any functions in your `single_stage_detector_yolo.ipynb` notebook, **they will not automatically be propagated to this notebook**. For changes to `single_stage_detector_yolo.ipynb` to be propagated to this notebook, you will need to:\n",
        "1. Make sure that you save your modified `single_stage_detector_yolo.ipynb` notebook (File > Save)\n",
        "2. Restart the runtime of this notebook (Runtime > Restart Runtime)\n",
        "3. Rerun all cells in this notebook (in particular the import cell below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xq5yHDeuklId",
        "colab": {}
      },
      "source": [
        "YOLO_NOTEBOOK_LINK = \"\"\n",
        "\n",
        "fcn_id = extract_drive_file_id(YOLO_NOTEBOOK_LINK)\n",
        "print('Google Drive file id: \"%s\"' % fcn_id)\n",
        "register_colab_notebooks({'single_stage_detector_yolo': fcn_id})\n",
        "\n",
        "from single_stage_detector_yolo import data_visualizer, FeatureExtractor\n",
        "from single_stage_detector_yolo import get_pascal_voc2007_data, pascal_voc2007_loader\n",
        "from single_stage_detector_yolo import coord_trans, GenerateGrid, GenerateAnchor, GenerateProposal\n",
        "from single_stage_detector_yolo import IoU, ReferenceOnActivatedAnchors   \n",
        "from single_stage_detector_yolo import DetectionSolver, DetectionInference\n",
        "\n",
        "print('Import successful!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MjJ3uyYBg3Lw"
      },
      "source": [
        "## Load PASCAL VOC 2007 data\n",
        "As in the previous notebook, we will use the PASCAL VOC 2007 dataset to train our object detection system.\n",
        "\n",
        "As in the previous notebook, we will subsample the dataset and wrap it in a DataLoader that can form minibatches for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MmEP5KQJzk0d",
        "colab": {}
      },
      "source": [
        "# uncomment below to use the mirror link if the original link is broken\n",
        "# !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
        "train_dataset = get_pascal_voc2007_data('/content', 'train')\n",
        "val_dataset = get_pascal_voc2007_data('/content', 'val')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OfwTGpZn1L5U",
        "colab": {}
      },
      "source": [
        "class_to_idx = {'aeroplane':0, 'bicycle':1, 'bird':2, 'boat':3, 'bottle':4,\n",
        "                'bus':5, 'car':6, 'cat':7, 'chair':8, 'cow':9, 'diningtable':10,\n",
        "                'dog':11, 'horse':12, 'motorbike':13, 'person':14, 'pottedplant':15,\n",
        "                'sheep':16, 'sofa':17, 'train':18, 'tvmonitor':19\n",
        "}\n",
        "idx_to_class = {i:c for c, i in class_to_idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XL-7Em_A1kdS",
        "colab": {}
      },
      "source": [
        "train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(0, 2500)) # use 2500 samples for training\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 10)\n",
        "val_loader = pascal_voc2007_loader(val_dataset, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZVYFJD32I_l",
        "colab": {}
      },
      "source": [
        "train_loader_iter = iter(train_loader)\n",
        "img, ann, _, _, _ = train_loader_iter.next()\n",
        "\n",
        "print('Resized train images shape: ', img[0].shape)\n",
        "print('Padded annotation tensor shape: ', ann[0].shape)\n",
        "print(ann[0])\n",
        "print('Each row in the annotation tensor indicates (x_tl, y_tl, x_br, y_br, class).')\n",
        "print('Padded with bounding boxes (-1, -1, -1, -1, -1) to enable batch loading. (You may need to run a few times to see the paddings)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X4WmocEyiXWa"
      },
      "source": [
        "## Visualize PASCAL VOC 2007\n",
        "Sample a couple of images and GT boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ld1s28Z4fyL5",
        "colab": {}
      },
      "source": [
        "# default examples for visualization\n",
        "fix_random_seed(0)\n",
        "batch_size = 3\n",
        "sampled_idx = torch.linspace(0, len(train_dataset)-1, steps=batch_size).long()\n",
        "\n",
        "# get the size of each image first\n",
        "h_list = []\n",
        "w_list = []\n",
        "img_list = [] # list of images\n",
        "MAX_NUM_BBOX = 40\n",
        "box_list = torch.LongTensor(batch_size, MAX_NUM_BBOX, 5).fill_(-1) # PADDED GT boxes\n",
        "\n",
        "for idx, i in enumerate(sampled_idx):\n",
        "  # hack to get the original image so we don't have to load from local again...\n",
        "  img, ann = train_dataset.__getitem__(i)\n",
        "  img_list.append(img)\n",
        "\n",
        "  all_bbox = ann['annotation']['object']\n",
        "  if type(all_bbox) == dict:\n",
        "    all_bbox = [all_bbox]\n",
        "  for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "    bbox = one_bbox['bndbox']\n",
        "    obj_cls = one_bbox['name']\n",
        "    box_list[idx][bbox_idx] = torch.LongTensor([int(bbox['xmin']), int(bbox['ymin']),\n",
        "      int(bbox['xmax']), int(bbox['ymax']), class_to_idx[obj_cls]])\n",
        "\n",
        "  # get sizes\n",
        "  img = np.array(img)\n",
        "  w_list.append(img.shape[1])\n",
        "  h_list.append(img.shape[0])\n",
        "\n",
        "w_list = torch.tensor(w_list, **to_float_cuda)\n",
        "h_list = torch.tensor(h_list, **to_float_cuda)\n",
        "box_list = torch.tensor(box_list, **to_float_cuda)\n",
        "resized_box_list = coord_trans(box_list, w_list, h_list, mode='p2a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v04D-gwEiWqY",
        "colab": {}
      },
      "source": [
        "# visualize GT boxes\n",
        "for i in range(len(img_list)):\n",
        "  valid_box = sum([1 if j != -1 else 0 for j in box_list[i][:, 0]])\n",
        "  data_visualizer(img_list[i], idx_to_class, box_list[i][:valid_box])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oRc7P-RvRZGZ"
      },
      "source": [
        "# Region Proposal Networks (RPN)\n",
        "The first stage in a Faster R-CNN object detector is the *Region Proposal Network (RPN)*. The RPN classifies a set of anchors as either containing an object or not, and also regresses from the position of the anchor box to a region proposal.\n",
        "\n",
        "The RPN is very similar to the single-stage detector we built in the previous notebook, except that it will not predict classification scores. We can therefore reuse many of the functions from the previous notebook in order to implement the RPN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etBYc7rbj35F"
      },
      "source": [
        "## Anchor\n",
        "We will use the exact same set of anchors as in the single-stage detector from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O5w-EUJekJj-",
        "colab": {}
      },
      "source": [
        "# Declare variables for anchor priors, a Ax2 Tensor where A is the number of anchors.\n",
        "# Hand-picked, same as our two-stage detector.\n",
        "anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]], **to_float_cuda)\n",
        "print(anchor_list.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7iHgTW0KTj13"
      },
      "source": [
        "## Activated (positive) and negative anchors\n",
        "When training the RPN, we compare the anchor boxes with the ground-truth boxes in order to determine a ground-truth label for the anchor boxes -- should each anchor predict object or background?\n",
        "\n",
        "We assign a positive label to two kinds of anchors:\n",
        "\n",
        "(i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or\n",
        "\n",
        "(ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors.\n",
        "\n",
        "Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample.\n",
        "\n",
        "We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oalEB2-Pa1zQ"
      },
      "source": [
        "We can implement anchor generation and matching to ground-truth by reusing the `GenerateGrid`, `GenerateAnchor`, `IoU`, and `ReferenceOnActivatedAnchors` functions from the previous notebook.\n",
        "\n",
        "Run the following to check the implementation from A5-1 (with your IoU function). You should see errors on the order of 1e-7 or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fK_USCuaXSzh",
        "colab": {}
      },
      "source": [
        "fix_random_seed(0)\n",
        "\n",
        "grid_list = GenerateGrid(w_list.shape[0])\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list)\n",
        "iou_mat = IoU(anc_list, resized_box_list)\n",
        "activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, \\\n",
        "  activated_anc_coord, negative_anc_coord = ReferenceOnActivatedAnchors(anc_list, resized_box_list, grid_list, iou_mat)\n",
        "\n",
        "expected_GT_conf_scores = torch.tensor([0.74538743, 0.72793430, 0.71128041, 0.70029843,\n",
        "                                        0.75670898, 0.76044953, 0.37116671, 0.37116671], **to_float_cuda)\n",
        "expected_GT_offsets = torch.tensor([[ 0.01633334,  0.11911901, -0.09431065,  0.19244696],\n",
        "                                    [-0.03675002,  0.09324861, -0.00250307,  0.25213102],\n",
        "                                    [-0.03675002, -0.15675139, -0.00250307,  0.25213102],\n",
        "                                    [-0.02940002,  0.07459889, -0.22564663,  0.02898745],\n",
        "                                    [ 0.11879997,  0.03208542,  0.20863886, -0.07974572],\n",
        "                                    [-0.08120003,  0.03208542,  0.20863886, -0.07974572],\n",
        "                                    [ 0.07699990,  0.28533328, -0.03459148, -0.86750042],\n",
        "                                    [ 0.07699990, -0.21466672, -0.03459148, -0.86750042]], **to_float_cuda)\n",
        "expected_GT_class = torch.tensor([ 6,  7,  7,  7, 19, 19,  6,  6], **to_long_cuda)\n",
        "print('conf scores error: ', rel_error(GT_conf_scores, expected_GT_conf_scores))\n",
        "print('offsets error: ', rel_error(GT_offsets, expected_GT_offsets))\n",
        "print('class prob error: ', rel_error(GT_class, expected_GT_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wPvX4TrgaLD8",
        "colab": {}
      },
      "source": [
        "# visualize the activated anchors\n",
        "anc_per_img = torch.prod(torch.tensor(anc_list.shape[1:-1]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Activated (positive) anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (activated_anc_ind >= idx * anc_per_img) & (activated_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} activated anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(activated_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Negative anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (negative_anc_ind >= idx * anc_per_img) & (negative_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} negative anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(negative_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XW_Zek3_dgfF"
      },
      "source": [
        "## Proposal module\n",
        "Similar to the Prediction Networks in A5-1, but for RPN you only need to predict the object proposal scores (from the *cls* layer) and bounding box offsets (from the *reg* layer), all of which are class-agnostic.\n",
        "\n",
        "![pred_scores2](https://miro.medium.com/max/918/1*wB3ctS9WGNmw6pP_kjLjgg.png)\n",
        "\n",
        "Note that here $k$ is essentially $A$. Image credit: Ren et al, \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\", NeurIPS 2015, https://arxiv.org/abs/1506.01497"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vMkvupmCdnYH",
        "colab": {}
      },
      "source": [
        "class ProposalModule(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=256, num_anchors=9, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_anchors != 0)\n",
        "    self.num_anchors = num_anchors\n",
        "    ##############################################################################\n",
        "    # TODO: Define the region proposal layer - a sequential module with a 3x3    #\n",
        "    # conv layer, followed by a Dropout (p=drop_ratio), a Leaky ReLU and         #\n",
        "    # a 1x1 conv.                                                                #\n",
        "    # HINT: The output should be of shape Bx(Ax6)x7x7, where A=self.num_anchors. #\n",
        "    #       Determine the padding of the 3x3 conv layer given the output dim.    #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def _extract_anchor_data(self, anchor_data, anchor_idx):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - anchor_data: Tensor of shape (B, A, D, H, W) giving a vector of length\n",
        "      D for each of A anchors at each point in an H x W grid.\n",
        "    - anchor_idx: int64 Tensor of shape (M,) giving anchor indices to extract\n",
        "\n",
        "    Returns:\n",
        "    - extracted_anchors: Tensor of shape (M, D) giving anchor data for each\n",
        "      of the anchors specified by anchor_idx.\n",
        "    \"\"\"\n",
        "    B, A, D, H, W = anchor_data.shape\n",
        "    anchor_data = anchor_data.permute(0, 1, 3, 4, 2).contiguous().view(-1, D)\n",
        "    extracted_anchors = anchor_data[anchor_idx]\n",
        "    return extracted_anchors\n",
        "\n",
        "  def forward(self, features, pos_anchor_coord=None, \\\n",
        "              pos_anchor_idx=None, neg_anchor_idx=None):\n",
        "    \"\"\"\n",
        "    Run the forward pass of the proposal module.\n",
        "\n",
        "    Inputs:\n",
        "    - features: Tensor of shape (B, in_dim, H', W') giving features from the\n",
        "      backbone network.\n",
        "    - pos_anchor_coord: Tensor of shape (M, 4) giving the coordinates of\n",
        "      positive anchors. Anchors are specified as (x_tl, y_tl, x_br, y_br) with\n",
        "      the coordinates of the top-left corner (x_tl, y_tl) and bottom-right\n",
        "      corner (x_br, y_br). During inference this is None.\n",
        "    - pos_anchor_idx: int64 Tensor of shape (M,) giving the indices of positive\n",
        "      anchors. During inference this is None.\n",
        "    - neg_anchor_idx: int64 Tensor of shape (M,) giving the indicdes of negative\n",
        "      anchors. During inference this is None.\n",
        "\n",
        "    The outputs from this module are different during training and inference.\n",
        "    \n",
        "    During training, pos_anchor_coord, pos_anchor_idx, and neg_anchor_idx are\n",
        "    all provided, and we only output predictions for the positive and negative\n",
        "    anchors. During inference, these are all None and we must output predictions\n",
        "    for all anchors.\n",
        "\n",
        "    Outputs (during training):\n",
        "    - conf_scores: Tensor of shape (2M, 2) giving the classification scores\n",
        "      (object vs background) for each of the M positive and M negative anchors.\n",
        "    - offsets: Tensor of shape (M, 4) giving predicted transforms for the\n",
        "      M positive anchors.\n",
        "    - proposals: Tensor of shape (M, 4) giving predicted region proposals for\n",
        "      the M positive anchors.\n",
        "\n",
        "    Outputs (during inference):\n",
        "    - conf_scores: Tensor of shape (B, A, 2, H', W') giving the predicted\n",
        "      classification scores (object vs background) for all anchors\n",
        "    - offsets: Tensor of shape (B, A, 4, H', W') giving the predicted transforms\n",
        "      for all anchors\n",
        "    \"\"\"\n",
        "    if pos_anchor_coord is None or pos_anchor_idx is None or neg_anchor_idx is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "    conf_scores, offsets, proposals = None, None, None\n",
        "    ############################################################################\n",
        "    # TODO: Predict classification scores (object vs background) and transforms#\n",
        "    # for all anchors. During inference, simply output predictions for all     #\n",
        "    # anchors. During training, extract the predictions for only the positive  #\n",
        "    # and negative anchors as described above, and also apply the transforms to#\n",
        "    # the positive anchors to compute the coordinates of the region proposals. #\n",
        "    #                                                                          #\n",
        "    # HINT: You can extract information about specific proposals using the     #\n",
        "    # provided helper function self._extract_anchor_data.                      #\n",
        "    # HINT: You can compute proposal coordinates using the GenerateProposal    #\n",
        "    # function from the previous notebook.                                     #\n",
        "    ############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    if mode == 'train':\n",
        "      return conf_scores, offsets, proposals\n",
        "    elif mode == 'eval':\n",
        "      return conf_scores, offsets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-6KpPF7bYJr"
      },
      "source": [
        "Run the following to check your implementation. You should see errors on the order of 1e-7 or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MX2JCaOf0768",
        "colab": {}
      },
      "source": [
        "# sanity check\n",
        "fix_random_seed(0)\n",
        "prop_module = ProposalModule(1280, drop_ratio=0).to(**to_float_cuda)\n",
        "features = torch.linspace(-10., 10., steps=3*1280*7*7, **to_float_cuda).view(3, 1280, 7, 7)\n",
        "conf_scores, offsets, proposals = prop_module(features, activated_anc_coord, \\\n",
        "              pos_anchor_idx=activated_anc_ind, neg_anchor_idx=negative_anc_ind)\n",
        "\n",
        "expected_conf_scores = torch.tensor([[-0.50843990,  2.62025023],\n",
        "                                     [-0.55775326, -0.29983672],\n",
        "                                     [-0.55796617, -0.30000290],\n",
        "                                     [ 0.17819080, -0.42211828],\n",
        "                                     [-0.51439995, -0.47708601],\n",
        "                                     [-0.51439744, -0.47703803],\n",
        "                                     [ 0.63225138,  2.71269488],\n",
        "                                     [ 0.63224381,  2.71290708]], **to_float_cuda)\n",
        "expected_offsets = torch.tensor([[ 1.62754285,  1.35253453, -1.85451591, -1.77882397],\n",
        "                                 [-0.33651856, -0.14402901, -0.07458937, -0.27201492],\n",
        "                                 [-0.33671042, -0.14398587, -0.07479107, -0.27199429],\n",
        "                                 [ 0.06847382,  0.21062726,  0.09334904, -0.02446130],\n",
        "                                 [ 0.16506940, -0.30296192,  0.29626080,  0.32173073],\n",
        "                                 [ 0.16507357, -0.30302414,  0.29625297,  0.32169008],\n",
        "                                 [ 1.59992146, -0.75236654,  1.66449440,  2.05138564],\n",
        "                                 [ 1.60008609, -0.75249159,  1.66474164,  2.05162382]], **to_float_cuda)\n",
        "\n",
        "print('conf scores error: ', rel_error(conf_scores[:8], expected_conf_scores))\n",
        "print('offsets error: ', rel_error(offsets, expected_offsets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dlO2IUCnt4zu"
      },
      "source": [
        "## Loss Function\n",
        "The confidence score regression loss is for both activated/negative anchors while the bounding box regression loss loss is for activated anchors only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "75QCAjEqt4zs"
      },
      "source": [
        "### Confidence score regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hguMsde4t4zj",
        "colab": {}
      },
      "source": [
        "def ConfScoreRegression(conf_scores, batch_size):\n",
        "  \"\"\"\n",
        "  Binary cross-entropy loss\n",
        "\n",
        "  Inputs:\n",
        "  - conf_scores: Predicted confidence scores, of shape (2M, 2). Assume that the\n",
        "    first M are positive samples, and the last M are negative samples.\n",
        "\n",
        "  Outputs:\n",
        "  - conf_score_loss: Torch scalar\n",
        "  \"\"\"\n",
        "  # the target conf_scores for positive samples are ones and negative are zeros\n",
        "  M = conf_scores.shape[0] // 2\n",
        "  GT_conf_scores = torch.zeros_like(conf_scores)\n",
        "  GT_conf_scores[:M, 0] = 1.\n",
        "  GT_conf_scores[M:, 1] = 1.\n",
        "\n",
        "  conf_score_loss = F.binary_cross_entropy_with_logits(conf_scores, GT_conf_scores, \\\n",
        "                                     reduction='sum') * 1. / batch_size\n",
        "  return conf_score_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uuVgug42t4zh"
      },
      "source": [
        "### Bounding box regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EcOYeku8t4zY",
        "colab": {}
      },
      "source": [
        "def BboxRegression(offsets, GT_offsets, batch_size):\n",
        "  \"\"\"\"\n",
        "  Use SmoothL1 loss as in Faster R-CNN\n",
        "\n",
        "  Inputs:\n",
        "  - offsets: Predicted box offsets, of shape (M, 4)\n",
        "  - GT_offsets: GT box offsets, of shape (M, 4)\n",
        "  \n",
        "  Outputs:\n",
        "  - bbox_reg_loss: Torch scalar\n",
        "  \"\"\"\n",
        "  bbox_reg_loss = F.smooth_l1_loss(offsets, GT_offsets, reduction='sum') * 1. / batch_size\n",
        "  return bbox_reg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3vfXnAcUbnMw"
      },
      "source": [
        "Run the following to check your implementation. You should see errors on the order of 1e-7 or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2eSleGX9yTeo",
        "colab": {}
      },
      "source": [
        "conf_loss = ConfScoreRegression(conf_scores, features.shape[0])\n",
        "reg_loss = BboxRegression(offsets, GT_offsets, features.shape[0])\n",
        "print('conf loss: {:.4f}, reg loss: {:.4f}'.format(conf_loss, reg_loss))\n",
        "\n",
        "loss_all = torch.tensor([conf_loss.data, reg_loss.data], **to_float_cuda)\n",
        "expected_loss = torch.tensor([8.55673981, 5.10593748], **to_float_cuda)\n",
        "\n",
        "print('loss error: ', rel_error(loss_all, expected_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AiPfXUHPupDE"
      },
      "source": [
        "## RPN module\n",
        "Implement Region Proposal Network. Should resemble the `SingleStageDetector` module from A5-1, but without the class prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yrnEwmgluq2Y",
        "colab": {}
      },
      "source": [
        "class RPN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # READ ONLY\n",
        "    self.anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]])\n",
        "    self.feat_extractor = FeatureExtractor()\n",
        "    self.prop_module = ProposalModule(1280, num_anchors=self.anchor_list.shape[0])\n",
        "\n",
        "  def forward(self, images, bboxes, output_mode='loss'):\n",
        "    \"\"\"\n",
        "    Training-time forward pass for the Region Proposal Network.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, 224, 224) giving input images\n",
        "    - bboxes: Tensor of ground-truth bounding boxes, returned from the DataLoader\n",
        "    - output_mode: One of 'loss' or 'all' that determines what is returned:\n",
        "      If output_mode is 'loss' then the output is:\n",
        "      - total_loss: Torch scalar giving the total RPN loss for the minibatch\n",
        "      If output_mode is 'all' then the output is:\n",
        "      - total_loss: Torch scalar giving the total RPN loss for the minibatch\n",
        "      - pos_conf_scores: Tensor of shape (M, 1) giving the object classification\n",
        "        scores (object vs background) for the positive anchors\n",
        "      - proposals: Tensor of shape (M, 4) giving the coordiantes of the region\n",
        "        proposals for the positive anchors\n",
        "      - features: Tensor of features computed from the backbone network\n",
        "      - GT_class: Tensor of shape (M,) giving the ground-truth category label\n",
        "        for the positive anchors.\n",
        "      - pos_anchor_idx: Tensor of shape (M,) giving indices of positive anchors\n",
        "      - neg_anchor_idx: Tensor of shape (M,) giving indices of negative anchors\n",
        "      - anc_per_image: Torch scalar giving the number of anchors per image.\n",
        "    \n",
        "    Outputs: See output_mode\n",
        "\n",
        "    HINT: The function ReferenceOnActivatedAnchors from the previous notebook\n",
        "    can compute many of these outputs -- you should study it in detail:\n",
        "    - pos_anchor_idx (also called activated_anc_ind)\n",
        "    - neg_anchor_idx (also called negative_anc_ind)\n",
        "    - GT_class\n",
        "    \"\"\"\n",
        "    # weights to multiply to each loss term\n",
        "    w_conf = 1 # for conf_scores\n",
        "    w_reg = 5 # for offsets\n",
        "\n",
        "    assert output_mode in ('loss', 'all'), 'invalid output mode!'\n",
        "    total_loss = None\n",
        "    conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = \\\n",
        "      None, None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of RPN.                                   #\n",
        "    # A few key steps are outlined as follows:                                   #\n",
        "    # i) Image feature extraction,                                               #\n",
        "    # ii) Grid and anchor generation,                                            #\n",
        "    # iii) Compute IoU between anchors and GT boxes and then determine activated/#\n",
        "    #      negative anchors, and GT_conf_scores, GT_offsets, GT_class,           #\n",
        "    # iv) Compute conf_scores, offsets, proposals through the region proposal    #\n",
        "    #     module                                                                 #\n",
        "    # v) Compute the total_loss for RPN which is formulated as:                  #\n",
        "    #    total_loss = w_conf * conf_loss + w_reg * reg_loss,                     #\n",
        "    #    where conf_loss is determined by ConfScoreRegression, w_reg by          #\n",
        "    #    BboxRegression. Note that RPN does not predict any class info.          #\n",
        "    #    We have written this part for you which you've already practiced earlier#\n",
        "    # HINT: Do not apply thresholding nor NMS on the proposals during training   #\n",
        "    #       as positive/negative anchors have been explicitly targeted.          #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    if output_mode == 'loss':\n",
        "      return total_loss\n",
        "    else:\n",
        "      return total_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img\n",
        "\n",
        "\n",
        "  def inference(self, images, thresh=0.5, nms_thresh=0.7, mode='RPN'):\n",
        "    \"\"\"\n",
        "    Inference-time forward pass for the Region Proposal Network.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, H, W) giving input images\n",
        "    - thresh: Threshold value on confidence scores. Proposals with a predicted\n",
        "      object probability above thresh should be kept. HINT: You can convert the\n",
        "      object score to an object probability using a sigmoid nonlinearity.\n",
        "    - nms_thresh: IoU threshold for non-maximum suppression\n",
        "    - mode: One of 'RPN' or 'FasterRCNN' to determine the outputs.\n",
        "\n",
        "    The region proposal network can output a variable number of region proposals\n",
        "    per input image. We assume that the input image images[i] gives rise to\n",
        "    P_i final propsals after thresholding and NMS.\n",
        "\n",
        "    NOTE: NMS is performed independently per-image!\n",
        "\n",
        "    Outputs:\n",
        "    - final_proposals: List of length B, where final_proposals[i] is a Tensor\n",
        "      of shape (P_i, 4) giving the coordinates of the predicted region proposals\n",
        "      for the input image images[i].\n",
        "    - final_conf_probs: List of length B, where final_conf_probs[i] is a\n",
        "      Tensor of shape (P_i,) giving the predicted object probabilities for each\n",
        "      predicted region proposal for images[i]. Note that these are\n",
        "      *probabilities*, not scores, so they should be between 0 and 1.\n",
        "    - features: Tensor of shape (B, D, H', W') giving the image features\n",
        "      predicted by the backbone network for each element of images.\n",
        "      If mode is \"RPN\" then this is a dummy list of zeros instead.\n",
        "    \"\"\"\n",
        "    assert mode in ('RPN', 'FasterRCNN'), 'invalid inference mode!'\n",
        "\n",
        "    features, final_conf_probs, final_proposals = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Predicting the RPN proposal coordinates `final_proposals` and        #\n",
        "    # confidence scores `final_conf_probs`.                                     #\n",
        "    # The overall steps are similar to the forward pass but now you do not need  #\n",
        "    # to decide the activated nor negative anchors.                              #\n",
        "    # HINT: Threshold the conf_scores based on the threshold value `thresh`.     #\n",
        "    # Then, apply NMS to the filtered proposals given the threshold `nms_thresh`.#\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    if mode == 'RPN':\n",
        "      features = [torch.zeros_like(i) for i in final_conf_probs] # dummy class\n",
        "    return final_proposals, final_conf_probs, features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmukyW51hRHa"
      },
      "source": [
        "## RPN solver\n",
        "In Faster R-CNN, the RPN is trained jointly with the second-stage network. However, to test our RPN implementation, we will first train just the RPN; this is basically a class-agnostic single-stage detector, that only classifies regions as object or background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6puWIsYQhQkW",
        "colab": {}
      },
      "source": [
        "RPNSolver = DetectionSolver # the same solver as in YOLO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xVQPPDVchAGQ"
      },
      "source": [
        "## RPN - Overfit small data\n",
        "First we will overfit the RPN on a small subset of the PASCAL VOC 2007 dataset. After training you should see a loss around or less than 3.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTObddiog9wJ",
        "colab": {}
      },
      "source": [
        "# monitor the training loss\n",
        "num_sample = 10\n",
        "small_dataset = torch.utils.data.Subset(train_dataset, torch.linspace(0, len(train_dataset)-1, steps=num_sample).long())\n",
        "small_train_loader = pascal_voc2007_loader(small_dataset, 10) # a new loader\n",
        "\n",
        "for lr in [1e-3]:\n",
        "  print('lr: ', lr)\n",
        "  rpn = RPN()\n",
        "  RPNSolver(rpn, small_train_loader, learning_rate=lr, num_epochs=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AOIazbsrFEVc"
      },
      "source": [
        "## RPN - Inference\n",
        "We will now visualize the predicted boxes from the RPN that we overfit to a small training sample. We will reuse the `DetectionInference` function from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o2wxjsnYFDrw",
        "colab": {}
      },
      "source": [
        "RPNInference = DetectionInference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "94vbKR2JJ-Mq",
        "colab": {}
      },
      "source": [
        "# visualize the output from the overfitted model on small dataset\n",
        "# the bounding boxes should be really accurate\n",
        "# ignore the dummy object class (in blue) as RPN does not output class!\n",
        "RPNInference(rpn, small_train_loader, small_dataset, idx_to_class, thresh=0.8, nms_thresh=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jKjv6JLMRj7s"
      },
      "source": [
        "# Faster R-CNN\n",
        "We have implemented the first half of Faster R-CNN, i.e., RPN, which is class-agnostic. Here, we briefly describe the second half Fast R-CNN.\n",
        "\n",
        "Given the proposals or region of interests (RoI) from RPN, we warp each region from CNN activation map to a fixed size 2x2 by using [RoI Align](https://arxiv.org/pdf/1703.06870.pdf). Essentially, the RoI feature is determined by bilinear interpolation over the CNN activation map. We meanpool the RoI feature over the spatial dimension (2x2).\n",
        "\n",
        "Finally, we classify the meanpooled RoI feature into class probabilities.\n",
        "\n",
        "For simplicity, our two-stage detector here differs from a full Faster R-CNN system in a few aspects.\n",
        "1. In a full implementation, the second stage of the network would predict a second set of offsets to transform the region proposal into a final predicted object bounding box. However we omit this for simplicity.\n",
        "2. In a full implementation, the second stage of the network should be able to reject negative boxes -- in other words, if we want to predict C different object categories then the final classification layer of the second stage would predict a distribution over C+1 categories, with an extra one for background. We omit this, as it requires extra bookeeping in the second stage about which proposals are positive / negative; so for simplicity our second stage will only predict a distribution over C categories, and we will assume that the RPN has filtered out all background regions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hdqlAcP6kRvA"
      },
      "source": [
        "## RoI Align\n",
        "We will use the `roi_align` function from `torchvision`. Usage see https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "umYbQz6Ct2bm"
      },
      "source": [
        "## Faster R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmSpMc1zakpJ",
        "colab": {}
      },
      "source": [
        "class TwoStageDetector(nn.Module):\n",
        "  def __init__(self, in_dim=1280, hidden_dim=256, num_classes=20, \\\n",
        "               roi_output_w=2, roi_output_h=2, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_classes != 0)\n",
        "    self.num_classes = num_classes\n",
        "    self.roi_output_w, self.roi_output_h = roi_output_w, roi_output_h\n",
        "    ##############################################################################\n",
        "    # TODO: Declare your RPN and the region classification layer (in Fast R-CNN).#\n",
        "    # The region classification layer is a sequential module with a Linear layer,#\n",
        "    # followed by a Dropout (p=drop_ratio), a ReLU nonlinearity and another      #\n",
        "    # Linear layer that predicts classification scores for each proposal.        #\n",
        "    # HINT: The dimension of the two Linear layers are in_dim -> hidden_dim and  #\n",
        "    # hidden_dim -> num_classes.                                                 #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self, images, bboxes):\n",
        "    \"\"\"\n",
        "    Training-time forward pass for our two-stage Faster R-CNN detector.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, H, W) giving input images\n",
        "    - bboxes: Tensor of shape (B, N, 5) giving ground-truth bounding boxes\n",
        "      and category labels, from the dataloader.\n",
        "\n",
        "    Outputs:\n",
        "    - total_loss: Torch scalar giving the overall training loss.\n",
        "    \"\"\"\n",
        "    total_loss = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of TwoStageDetector.                      #\n",
        "    # A few key steps are outlined as follows:                                   #\n",
        "    # i) RPN, including image feature extraction, grid/anchor/proposal           #\n",
        "    #       generation, activated and negative anchors determination.            #\n",
        "    # ii) Perform RoI Align on proposals and meanpool the feature in the spatial #\n",
        "    #     dimension.                                                             #\n",
        "    # iii) Pass the RoI feature through the region classification layer which    #\n",
        "    #      gives the class probilities.                                          #\n",
        "    # iv) Compute class_prob through the prediction network and compute the      #\n",
        "    #     cross entropy loss (cls_loss) between the prediction class_prob and    #\n",
        "    #      the reference GT_class. Hint: Use F.cross_entropy loss.               #\n",
        "    # v) Compute the total_loss which is formulated as:                          #\n",
        "    #    total_loss = rpn_loss + cls_loss.                                       #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return total_loss\n",
        "\n",
        "  def inference(self, images, thresh=0.5, nms_thresh=0.7):\n",
        "    \"\"\"\"\n",
        "    Inference-time forward pass for our two-stage Faster R-CNN detector\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, H, W) giving input images\n",
        "    - thresh: Threshold value on NMS object probabilities\n",
        "    - nms_thresh: IoU threshold for NMS in the RPN\n",
        "\n",
        "    We can output a variable number of predicted boxes per input image.\n",
        "    In particular we assume that the input images[i] gives rise to P_i final\n",
        "    predicted boxes.\n",
        "\n",
        "    Outputs:\n",
        "    - final_proposals: List of length (B,) where final_proposals[i] is a Tensor\n",
        "      of shape (P_i, 4) giving the coordinates of the final predicted boxes for\n",
        "      the input images[i]\n",
        "    - final_conf_probs: List of length (B,) where final_conf_probs[i] is a\n",
        "      Tensor of shape (P_i,) giving the predicted probabilites that the boxes\n",
        "      in final_proposals[i] are objects (vs background)\n",
        "    - final_class: List of length (B,), where final_class[i] is an int64 Tensor\n",
        "      of shape (P_i,) giving the predicted category labels for each box in\n",
        "      final_proposals[i].\n",
        "    \"\"\"\n",
        "    final_proposals, final_conf_probs, final_class = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Predicting the final proposal coordinates `final_proposals`,        #\n",
        "    # confidence scores `final_conf_probs`, and the class index `final_class`.  #\n",
        "    # The overall steps are similar to the forward pass but now you do not need #\n",
        "    # to decide the activated nor negative anchors.                             #\n",
        "    # HINT: Use the RPN inference function to perform thresholding and NMS, and #\n",
        "    # to compute final_proposals and final_conf_probs. Use the predicted class  #\n",
        "    # probabilities from the second-stage network to compute final_class.       #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return final_proposals, final_conf_probs, final_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RFZ49wox4MYn"
      },
      "source": [
        "## Overfit small data\n",
        "We will now overfit the full Faster R-CNN network on a small subset of the training data. After training you should see a final loss less than 4.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WbxeAJq0zc3F",
        "colab": {}
      },
      "source": [
        "# monitor the training loss\n",
        "\n",
        "lr = 1e-3\n",
        "detector = TwoStageDetector()\n",
        "DetectionSolver(detector, small_train_loader, learning_rate=lr, num_epochs=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_SWA1DbG47ln"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gp_Hmt-Km5bl",
        "colab": {}
      },
      "source": [
        "# visualize the output from the overfitted model on small dataset\n",
        "# the bounding boxes should be really accurate\n",
        "DetectionInference(detector, small_train_loader, small_dataset, idx_to_class, thresh=0.8, nms_thresh=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sr7wNngy4oZf"
      },
      "source": [
        "## Train a net\n",
        "Now it's time to train the full Faster R-CNN model on a larger subset of the the training data. We will train for 50 epochs; this should take about 35 minutes on a K80 GPU. You should see a total loss less than 3.0.\n",
        "\n",
        "(Optional) If you train the model longer (e.g., 100 epochs), you should see a better mAP. But make sure you revert the code back for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X1k1rx1f4sTE",
        "colab": {}
      },
      "source": [
        "# monitor the training loss\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 100) # a new loader\n",
        "\n",
        "num_epochs = 50\n",
        "lr = 5e-3\n",
        "frcnn_detector = TwoStageDetector()\n",
        "DetectionSolver(frcnn_detector, train_loader, learning_rate=lr, num_epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qERg85Ip7hwu",
        "colab": {}
      },
      "source": [
        "# (optional) load/save checkpoint\n",
        "# torch.save(frcnn_detector.state_dict(), 'frcnn_detector.pt') # uncomment to save your checkpoint\n",
        "# frcnn_detector.load_state_dict(torch.load('frcnn_detector.pt')) # uncomment to load your previous checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KhWZT-ztEaqm"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J7ArGiLTnHta",
        "colab": {}
      },
      "source": [
        "# visualize the same output from the model trained on the entire training set\n",
        "# some bounding boxes might not make sense\n",
        "DetectionInference(frcnn_detector, small_train_loader, small_dataset, idx_to_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ETU6ev7aydIY"
      },
      "source": [
        "## Evaluation\n",
        "Compute mean Average Precision (mAP). Introduction on mAP see lecture slides (p46-57): https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture15.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8iGOlxhyvO3z"
      },
      "source": [
        "Run the following to evaluate your detector on the PASCAL VOC validation set. You should see mAP at around 16% or above.\n",
        "\n",
        "(Optional) If you train the model longer (e.g., 100 epochs), you should see a better mAP. But make sure you revert the code back for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FvDb7uwqyhAK",
        "colab": {}
      },
      "source": [
        "!rm -r mAP/input/*\n",
        "DetectionInference(frcnn_detector, val_loader, val_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3)\n",
        "# DetectionInference(frcnn_detector, train_loader, train_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3) # uncomment to see training mAP\n",
        "!cd mAP && python main.py"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}